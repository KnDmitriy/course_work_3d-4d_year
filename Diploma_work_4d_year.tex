\documentclass[bachelor, och, diploma ]{SCWorks}
% параметр ---тип обучения ---одно из значений:
%    spec     ---специальность
%    bachelor ---бакалавриат (по умолчанию)
%    master   ---магистратура
% параметр ---форма обучения ---одно из значений:
%    och   ---очное (по умолчанию)
%    zaoch ---заочное
% параметр ---тип работы ---одно из значений:
%    referat    ---реферат
%    coursework ---курсовая работа (по умолчанию)
%    diploma    ---дипломная работа
%    pract      ---отчет по практике
%    pract      ---отчет о научно-исследовательской работе
%    autoref    ---автореферат выпускной работы
%    assignment ---задание на выпускную квалификационную работу
%    review     ---отзыв руководителя
%    critique   ---рецензия на выпускную работу
% параметр ---включение шрифта
%    times    ---включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage{amsmath}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage{caption}
\usepackage[english,russian]{babel}
\captionsetup[figure]{font= normalsize, labelfont=normalsize}
\DeclareUnicodeCharacter{00A0}{ }

\usepackage[colorlinks=true]{hyperref}
\usepackage{float}
\usepackage{caption}
\captionsetup[figure]{font= normalsize, labelfont=normalsize}


\begin{document}

% Кафедра (в родительном падеже)
\chair{Дискретной математики и информационных технологий}

% Тема работы
\title{Разработка приложения для частотного анализа новостных сообщений}

% Курс
\course{4}

% Группа
\group{421}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код ---наименование
%\napravlenie{02.03.02 "-----Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "-----Математическое обеспечение и администрирование информационных систем}
\napravlenie{09.03.01 "---Информатика и вычислительная техника}
%\napravlenie{09.03.04 "-----Программная инженерия}
%\napravlenie{10.05.01 "-----Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Конорова Дмитрия Аркадьевича}

% Заведующий кафедрой
\chtitle{доцент, к.\,ф.-м.\,н.} % степень, звание
\chname{Л.\,Б.\,Тяпаев}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,э.\,н.} %должность, степень, звание
\saname{Г.\,Ю.\,Чернышова}


% Семестр (только для практики, для остальных
% типов работ не используется)
% \term{}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
\duration{}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
\practStart{}
\practFinish{}

% Год выполнения отчета
\date{2025}




\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию ---нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents


% Раздел "Обозначения и сокращения". Может отсутствовать в работе


% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
\intro

Понятие "ключевое слово" уже давно получило точное определение.
Бойс и др. [7] назвали его !?суррогатом?!, который представляет тему
или содержание документа, что, в свою очередь, порождает другой
вопрос: что такое тема или содержание? Что в равной степени неуловимо. Существует две основных научных школы: одна в области терминологии (TS), другая в области поиска информации (IR). Они пересекались по мере продвижения в своих научных изысканиях. Терминологи, как правило, занимаются поиском терминов, которые являются
специфичными для конкретной технической области. Это полезно для систематизации
знаний, относящихся к этой области, в то время как люди, занимающиеся поиском информации, больше сосредоточены на идентификации терминов (которые
они называют терминами индексации), способных различать
документы для улучшения поиска документов.





В современном мире человека окружает большое количество информации. Поток данных становится всё более насыщенным, и с каждым днём ориентироваться в нём становится всё сложнее.

Чтобы эффективно воспринимать и использовать это изобилие информации, необходимо уметь проводить её обработку, анализировать тексты, выявлять главные идеи и скрытые взаимосвязи. 

(что интересует людей и наиболее актуально для общества.) 

Была создана программа, которая значительно облегчает этот процесс. Была создана программа, которая помогает на основе использования слов в текстах выделять наиболее обсуждаемые темы в средствах массовой информации (СМИ) (наиболее актуальные для общества). 









\section{Методы извлечения ключевых слов}
(выделять ключевые слова, сравнивать ... (косинусная мера) и выделять активно развивающиеся тренды (строить тренд-карты). )

(Извлечение ключевых слов и нахождение ключевых слов являются синонимами в данном контексте. Но извлечение - это нахождение и запоминание.)



Интерпретация тренд-карты для Самарской области за 2021-2023 года.
Я думаю, что термины со значением динамики от -10 до 10 (или другой диапазон) и с большим значением значимости можно отнести к стабильно обсуждаемым темам в СМИ.

\textbf{От 16.09.2024}
https://aclanthology.org/2021.hackashop-1.6/ - статья 2021 года. 
"Exploring Linguistically-Lightweight Keyword Extraction Techniques for
Indexing News Articles in a Multilingual Set-up".
В ней есть третий раздел "Methods", который можно полностью скопировать для работы и перевести. Особенность статьи - поиск и сравнение алгоритмов для 7 языков.

https://link.springer.com/article/10.1007/s42979-022-01481-7 - статья 2022 года. "Keyword Extraction: A Modern Perspective".
Эта статья намного подробнее предыдущей, в ней проводится более подробное рассмотрение алгоритмов и их более четкая классификация на: статистические, лингвистические, графовые и на алгоритмы с использованием машинного обучения. Проводится сравнение эффективности алгоритмов. Эта статья больше предыдущей. "Мы рассмотрим историю поиска ключевых
слов за последние 50 лет, отмечая различия и сходства между методами, появившимися за это время," - написано в этой статье.

https://www.mdpi.com/2076-3417/13/12/7228 - статья 2023 года. "Unlocking the Potential of Keyword Extraction: The Need for Access to High-Quality Datasets"
В этой статье рассматриваются и сравниваются алгоритмы KeyBERT, YAKE и RAKE. В двух предыдущих статья рассматриваемых алгоритмов намного больше (не меньше 8).

https://www.sciencedirect.com/science/article/abs/pii/S0957417422018607 - статья 2022 года. 
"Extracting keywords of educational texts using a novel mechanism based on linguistic approaches and evolutive graphs".
К ней нет бесплатного доступа.

https://link.springer.com/chapter/10.1007/978-981-99-6706-3_30 - статья 2023 года. 
"User Story-Based Automatic Keyword Extraction using Algorithms and Alalysis". 
Не имеет бесплатного доступа.





\textbf{От 03.02.2025}
Статья "Back to the Basics: A Quantitative Analysis of Statistical and
Graph-Based Term Weighting Schemes for Keyword Extraction",
https://aclanthology.org/2021.emnlp-main.638.pdf, статья ноября 2021 года.



\subsection{Статистические методы}
Статистическая модель оценивает важность
на основе статистики на уровне слов или поверхностных характеристик,
таких как частота встречаемости или длина слова.
Простым методом извлечения ключевых слов может быть
простое использование частоты использования термина (tf) в качестве функции оценки для каждого слова, которая, как правило, работает достаточно хорошо.
Однако при таком простом измерении может быть упущена важная
информация, такая как относительная важность
данного слова в корпусе. Например,
такие союзы, как и, а, или такая частица, как не, как правило, очень
часто встречающиеся в текстовом тексте. Однако они едва ли представляют собой ключевое слово в данном текстовом документе. С этой
целью были предложены различные варианты, которые
мы резюмируем в двух основных вариантах: tf-idf  и лексическая специфика.

\subsubsection{TF-IDF}

Пусть \( t \) – слово, \( d \) – документ, \( D \) – коллекция документов.
\[
\text{tf}(t,d) = \frac{n_t}{\sum_{k} n_k},
\]
где \( n_t \) – это число вхождений слова \( t \) в документ \( d \), а в знаменателе — общее число слов в данном документе.

Обозначим
\[
\text{idf}(t,D) = \ln \left( \frac{|D| + 1}{|\{d_i \in D \mid t \in d_i\}| + 1} + 1 \right),
\]
где \( |D| \) — число документов в коллекции; \( |\{d_i \in D \mid t \in d_i\}| \) — число документов из коллекции \( D \), в которых встречается \( t \).

Тогда
\[
\text{tf-idf}(t,d,D) = \text{tf}(t,d) \cdot \text{idf}(t,D).
\]

Для каждого слова \( t_i \) из документа \( d_j \) вычисляется значение \( \text{tf-idf}\-(t_i,d_j,D) \). Формируется матрица \( V \), состоящая из элементов \( v_{ij} = \text{tf-idf}(t_j, d_i, D) \), \( i = 1, \ldots, n \), \( j = 1, \ldots, m \), где \( n \) — количество уникальных слов во всех документах из коллекции документов \( D \), \( m = |D| \).

Чтобы привести несколько примеров статистических моделей, основанных на tf-idf и его производных, в контексте извлечения ключевых слов, KP-miner (Elbeltagy и Rafea, 2013) использует tf-idf, длину слова и абсолютное положение слова в документе для определения показателя важности, в то время как RAKE (Rose et al., 2010) использует степень термина, количество различных слов, с которыми он сочетается, разделённое на tf. Недавно, YAKE (Campos et al., 2020) установил надежные базовые показатели для общедоступных наборов данных, объединив различные статистические характеристики, включая структуру, расположение предложений, частоту употребления термина/предложения и дисперсию терминов.

\subsubsection{Keyphrase Miner (KPMiner)}
\textbf{Первый вариант описания (KPMiner)}

\textit{Перевод описания метода KPMiner из статьи "A Comparative Assessment of Unsupervised Keyword Extraction Tools". Сайт: \url{https://ieeexplore.ieee.org/document/10363189?denied=}}

El-Beltagy и Rafea предложили метод обучения без учителя (unsupervised) KPMiner, который использует модифицированную версию TF-IDF и работает с n-граммами [59]. Предложенный метод состоит из трех основных этапов, включая выбор ключевых слов-кандидатов, расчет веса ключевых слов-кандидатов и уточнение выбранных ключевых слов. После удаления пунктуации и стоп-слов на этапе выбора ключевых слов-кандидатов KPMiner добавляет две новые статистические характеристики. 
\begin{enumerate}
    \item Чтобы слово стало ключевым словом-кандидатом, оно должно встречаться не менее \textit{k} раз в документе, из которого будут извлечены ключевые слова, и это называется наименьшей допустимой частотой встречаемости.
    \item Слово не будет считаться ключевым словом и будет отфильтровано, если оно встречается в длинном документе после определенной пороговой позиции отсечения.
\end{enumerate}

На этапе расчета веса ключевых слов-кандидатов KPMiner использует повышающий коэффициент для составных ключевых слов, чтобы сбалансировать смещение TF-IDF в сторону одиночных ключевых слов, поскольку одиночные ключевые слова имеют тенденцию получать более высокие оценки из-за потенциально более высокой частоты встречаемости. На заключительном этапе ключевые слова уточняются, чтобы вернуть \textit{m} ключевых слов с наивысшей оценкой.



\textbf{Второй вариант описания KPMiner}

Анализатор ключевых фраз (KP-Miner) опирается на частоту и позицию ключевых слов-кандидатов, представляющих собой n-граммы слов без знаков препинания, которые не начинаются и не заканчиваются стоп-словами. Он также учитывает вес ключевых слов, состоящих из нескольких токенов, как предложено Эль-Белтаги и Рафеа в 2009 году.

$s(k) = freq(k)\cdot max( \frac{|K|}{\alpha \cdot |K_m|}, \omega) \cdot \frac{1}{AvgPos(k)}$ , 
где freq(k), K, $K_m$ обозначают частоту k, набор всех ключевых слов-кандидатов и набор всех ключевых слов-кандидатов с несколькими токенами, соответственно, тогда как $\alpha$ и $\omega$ - это две константы корректировки веса, а AvgPos(k) обозначает среднее положение ключевого слова в тексте в терминах областей, разделенных знаками препинания.KP-Miner также имеет специальный параметр отсечения, который устанавливает количество токенов, после которого, если ключевое слово появляется впервые, оно отфильтровывается и не считается подходящим.
Наша версия KP-Miner не содержит исходных данных, отличных от оригинальной (El-Beltagy
и Rafea, 2009), из-за нашего многоязычного контекста
и спецификацию задания (см. раздел 2). Наконец, KP-Miner просматривает кандидатов, занявших n первых мест в рейтинге
, и удаляет тех, которые являются составными частями
других, и соответствующим образом корректирует баллы. На
основании эмпирических наблюдений конкретные параметры,
а именно $\alpha$, $\omega$ и пороговое значение, были установлены на 1,0, 3,0 и
1000 соответственно.





KP-Miner — это система для извлечения ключевых фраз из текста, которая работает в три этапа:  
\begin{enumerate}
    \item Выбор кандидатов в ключевые фразы.
    \item Расчёт веса кандидатов.
    \item Уточнение списка ключевых фраз.
\end{enumerate}

На этапе выбора кандидатов применяются правила для выделения потенциальных ключевых фраз. Кандидатами считаются последовательности слов, которые не разделены знаками препинания или стоп-словами (например, \textquotedblи\textquotedbl, \textquotedblа\textquotedbl, \textquotedblно\textquotedbl). 

Для фильтрации лишних кандидатов применяются два дополнительных условия:  
\begin{itemize}
    \item фраза должна встречаться в документе не менее n раз (по умолчанию n = 3). Для коротких документов это значение уменьшается;
    \item фраза должна впервые появиться в документе до определённого порога (например, после первых 400 слов). Фразы, появившиеся позже, игнорируются.  
\end{itemize}

Процесс извлечения фраз выполняется в два этапа:  

\begin{enumerate}
    \item Сканирование текста до встречи стоп-слова или знака препинания. Все возможные n-граммы (последовательности слов длиной от 1 до длины \textit{\textbf{фразы}}) сохраняются в исходной и \textit{\textbf{стеммированной}} формах. 
    \item Повторное сканирование текста для поиска наиболее длинных последовательностей, удовлетворяющих условиям.  
\end{enumerate}



Для определения значимости фраз используется модифицированная версия модели TF-IDF. Поскольку TF-IDF плохо работает с составными фразами (из-за их меньшей частоты по сравнению с отдельными словами), вводится коэффициент усиления (boosting factor).  

Формула для расчёта веса фразы:  
\[ w_{ij} = tf_{ij} \times idf \times B_i \times Pf \]  
Где:  
\begin{itemize}
    \item \( tf_{ij} \) — частота фразы в документе;
    \item \( idf \) — обратная документная частота (для составных фраз всегда равна константе);
    \item \( B_i \) — коэффициент усиления для документа;
    \item \( Pf \) — фактор позиции фразы в документе (если используется).  
\end{itemize}

На этапе уточнения списка ключевых фраз устраняется перекрытие в подсчёте частот для фраз и их подфраз. Например, если фраза "избыточный вес тела" встречается 3 раза, а её подфраза "вес тела" — 5 раз, то частота подфразы уменьшается на частоту основной фразы.  

После уточнения веса пересчитываются, и формируется итоговый список ключевых фраз, отсортированный по убыванию веса. Этот шаг является опциональным, но его использование улучшает качество извлечённых фраз.  


KP-Miner выделяет ключевые фразы, учитывая их частоту, позицию в тексте и длину, а также применяет дополнительные шаги для повышения точности результатов.




\subsubsection{YAKE}
\textit{Перевод описания метода YAKE из статьи "A Comparative Assessment of Unsupervised Keyword Extraction Tools". Сайт: \url{https://ieeexplore.ieee.org/document/10363189?denied=}}

Yet Another Keyword Extractor (YAKE) - еще один известный метод извлечения ключевых слов без учителя (unsupervised), который использует статистические признаки из одного документа, не опираясь на корпус документов, для извлечения наиболее важных ключевых слов [44]. Существенное отличие YAKE от KPMiner  заключается в том, что он вводит новый разнообразный набор признаков, отражающих характеристики каждого слова [60]. 
\begin{enumerate}
    \item Регистр термина (TCase ) отражает чувствительность слов-кандидатов к регистру.
    \item Положение термина (TPosition ) отражает, что слова, которые появляются в первых предложениях текста, имеют более высокие значения, чем слова, которые появляются позже.
    \item Нормализация частоты термина (TFNorm ) отражает, что значимость слова-кандидата увеличивается с частотой слова-кандидата. Однако для предотвращения смещения в сторону высокой частоты в длинных документах требуется нормализация. 
    \item Связь термина с контекстом (TRel ) показывает, сколько разных слов присутствует по обе стороны от слова-кандидата.
    \item Разные предложения термина (TSentence ) отражает идею о том, что слова-кандидаты, которые появляются в разных предложениях, с большей вероятностью будут значимыми.
\end{enumerate}   

После подсчета баллов по всем признакам вычисляется оценка для каждого слова по следующей формуле:

\begin{equation*} S\left ({t}\right)=\,\,\frac {T_{Rel}\ast ~T_{Position}}{T_{Case}+\,\,\frac {TF_{Norm}}{T_{Rel}}+\,\,\frac {T_{Sentence}}{T_{Rel}}} \tag{8}\end{equation*}

Итоговая оценка каждого слова-кандидата рассчитывается с помощью модели n-грамм следующим образом:

\begin{equation*} S\left ({kw}\right)=\,\,\frac {\prod _{t~\in k w}{S(t)}}{KF\left ({kw}\right)\ast \left({1+\,\,\sum _{t~\in k w} S\left ({t}\right)}\right)} \tag{9}\end{equation*}

где \textit{kw} - ключевая слово-кандидат из n-граммы, а \textit{KF} - частота встречаемости ключевого слова-кандидата. Чем меньше балл ключевой фразы-кандидата, тем более релевантным будет ключевая фраза-кандидат.



\subsubsection{RAKE}

В тексте определяются кандидаты в ключевые слова (ККС). Разделителями при разбиении являются стоп-слова и знаки пунктуации. После этого для каждого слова w определяются характеристики: степень d(w) и частота f(w). 
При этом f(w) определяется как частота встречаемости слова w в документе. 
Степень d(w) определяется так:
$d(w)= \sum_{w\in keyword}^{} (length(keyword) - 1)$
Где \textit{keyword} – ККС, \textit{length(keyword)} – количество слов в ККС.

.......

\subsection{Графовые методы}
Идея извлечения ключевых слов на основе графа возникла из алгоритма Google PageRank [62], основанного на базовом предположении, что большее количество реберных связей в графе показывает более значимые слова-кандидаты. Общий подход к извлечению ключевых слов на основе графов начинается с предварительной обработки текста, когда текст очищается, маркируется и разбивается на слова или фразы. Затем строится граф совпадений, узлы которого представляют слова или фразы, а ребра - взаимосвязи совпадений. Узлы на графике оцениваются с целью определения их важности, а узлы, занявшие первое место, рассматриваются как ключевые слова. Методы, основанные на графах, фиксируют семантические связи между словами в документе, поэтому они могут обеспечить более полное и контекстуализированное понимание содержания документа.

\textbf{\textit{TODO: Наверное, стоит добавитьописание алгоритма PageRank, так как далее я на него ссылаюсь}}


\subsubsection{TextRank}
\textit{Перевод описания метода KPMiner из статьи "A Comparative Assessment of Unsupervised Keyword Extraction Tools". Сайт: \url{https://ieeexplore.ieee.org/document/10363189?denied=}}

TextRank - это первый метод ранжирования релевантности предложений или ключевых слов в текстовом документе с использованием алгоритма PageRank на основе графов [63]. Метод начинается с создания графа текстового документа. Каждое предложение или ключевое слово представлено в виде узла на графе, а ребра между узлами показывают, насколько схожи связанные предложения или ключевые слова друг с другом. Чтобы предотвратить чрезмерный рост графа, он учитывает только 1-граммы и в конечном итоге восстанавливает ключевые фразы, состоящее из нескольких слов, на этапе постобработки. В отличие от PageRank, он учитывает вес между двумя узлами, поскольку графы строятся на основе контекста естественного языка. Он определяет формулу, которая интегрирует веса для расчета весовой оценки (\textit{WS}) узла $V_i$ следующим образом:

\begin{equation*} WS\left ({V_{i}}\right) \!= \! \left ({1\!-\!d}\right) \!+\! d\ast ~\sum _{V_{j}~\in ~In\left ({V_{i}}\right)~}{\frac {w_{ji}}{\sum _{V_{k}~\in ~Out\left ({V_{j}}\right)} w_{jk}}~WS(V_{j})} \tag{10}\end{equation*}

где \textit{d} - понижающий коэффициент, \textit{w} - вес, $In(V_i)$ - набор узлов, которые указывают на узел $V_i$, а $Out(V_i)$ - набор узлов, на которые указывает узел $V_i$.


\subsubsection{SingleRank}
\textit{Перевод описания метода из статьи "A Comparative Assessment of Unsupervised Keyword Extraction Tools". Сайт: \url{https://ieeexplore.ieee.org/document/10363189?denied=}}

Wan и Xiao предложили основанный на графах метод под названием Single Rank, который по сути похож на TextRank с некоторыми существенными отличиями [64]. Метод работает путем построения двудольного графа, в котором один набор узлов представляет документы, а другой набор - слова в документах. Ребра, соединяющие узлы, представляют собой совпадения слов в документах. Затем SingleRank использует итеративный процесс для вычисления оценок важности для каждого документа на основе оценок слов, которые встречаются в документе, и оценок документов, в которых эти слова содержатся.

\textit{\textbf{Как происходит оценка важности документов и фраз кандидатов?}}


\subsubsection{PositionRank}
\textit{Перевод описания метода из статьи "A Comparative Assessment of Unsupervised Keyword Extraction Tools". Сайт: \url{https://ieeexplore.ieee.org/document/10363189?denied=}}

Ранжирование позиций - это основанный на графах метод KE, который вводит информацию о местоположении с учетом того, что важные ключевые слова появляются ранее в документе [65]. В методе используется модифицированный подход PageRank, при котором узлы на графе указывают ключевые слова, а ребра представляют сходство между парами ключевых слов на основе их положения и совместной встречаемости. Position Rank присваивает вес на основе положения слов и вычисляет рейтинг узлов следующим образом:

\begin{align*} \widetilde {p}&= \left [{\frac {p_{1}}{p_{1}+p_{2}+\ldots +p_{\left |{V}\right |}}, }\right. \\ &\left.{ \quad \ldots, \frac {p_{\left |{V}\right |}}{p_{1}+p_{2}+\ldots +p_{\left |{V}\right |}}~}\right] \tag{11}\\ S\left ({v_{i}}\right)&= \left ({1-d}\right).~\widetilde {p_{i}} \\ &\quad + d\ast ~\sum _{V_{j}~\in ~Adj\left ({V_{i}}\right)~}{\frac {w_{ji}}{\sum _{V_{k}~\in ~Adj\left ({V_{j}}\right)} w_{jk}} S(V_{j})} \tag{12}\end{align*}

где p - вектор длины |V|, и это указывает на то, что, находясь в узле vi , случайное блуждание может перейти к любому другому узлу на графике с равной вероятностью.

\textbf{\textit{Скупое и неполное описание метода.  TODO: Добавить или изменить описание метода PositionRank}}


\subsubsection{Topic Rank}
\textit{Перевод описания метода из статьи "A Comparative Assessment of Unsupervised Keyword Extraction Tools". Сайт: \url{https://ieeexplore.ieee.org/document/10363189?denied=}}

Topic Rank - это основанный на графах метод KE, который использует тематическое представление документа для извлечения важных ключевых слов из документа [55]. Сначала метод идентифицирует темы-кандидаты путем кластеризации связанных ключевых слов в документе, используя вариант метода TextRank для определения наиболее важных ключевых слов в каждом кластере. Он строит график, в котором узлы представляют идентифицированные темы, а ребра - степень сходства между темами. Сходство между темами рассчитывается на основе сходства между ключевыми словами, составляющими каждую тему. Алгоритм иерархической агломеративной кластеризации (HAC) [66] используется для автоматической группировки похожих ключевых слов по темам. TopicRank особенно полезен для выявления важных тем, которые могут не охватываться отдельными ключевыми словами.


\section{}

Исходные данные ... 

Возьмем в качестве срезов двумерный массив с элементами $T_{ij}$. Каждый такой элемент является конкатенацией текстов, выпущенных за определенный период времени средствами массовой информации в сообществе VK одного из регионов. Индекс i определяет период, индекс j определяет регион. При этом $T_{ij} = t_{ij}^1 \cdot \dotsc \cdot t_{ij}^l \cdot \dotsc \cdot t_{ij}^k$, где $t_{ij}^l$ - один новостной пост в сообществе VK, знак $\cdot$ означает конкатенацию текстов.

RAKE 

Сначала текст разбивается на фразы-кандидаты. Разделителями при разбиении являются стоп-слова и знаки пунктуации. После этого для каждого слова высчитываются характеристики: степень ($d$) и частота ($f$). f = частоте встречаемости слова в документе. d = сумме длин фраз-кандидатов, в которые входит данное слово. Для каждого слова производится вычисление $score = \frac{d}{f}$. Характеристика score для ключевой фразы высчитывается как сумма характеристик score каждого слова, входящего в ключевую фразу. Фразы с наибольшим значением score считаются ключевыми.

Пусть $RAKE(T_{ij})$ - таблица, первым столбцом которой являются все кандидаты в ключевые фразы, полученные при анализе текстов $T_{ij}$, а во втором характеристика RAKE для каждого из кандидатов. 
Динамика для каждого слова высчитывается следующим образом. $RAKE(T_{kj}) - RAKE(T_{1j})$, где k - индекс последнего периода, RAKE(

Для каждой фразы-кандидата 


-------------------------------------------------------------


RAKE запускает извлечение ключевых слов из документа, разбирая его текст на набор подходящих ключевых фраз. 

Сначала текст документа разбивается на массив слов с помощью указанных разделителей слов. В данном случае разделителем является пробел. Ключевой особенностью RAKE является использование стоп-слов (например, предлогов, союзов) и знаков препинания в качестве разделителей для формирования фраз-кандидатов.

Затем этот массив разбивается на последовательности смежных слов с разделителями фраз стоп-словами и знаками пунктуации. 

После определения каждой ключевой фразы-кандидата и составления графика совпадений слов (показанного на рисунке 1.3) для каждого ключевого слова-кандидата вычисляется оценка, которая определяется как сумма оценок входящих в него слов. Мы оценили несколько параметров для расчета оценки слов, основываясь на степени и частоте значений слов на графике: (1) частота слова (частота (w)), (2) степень слова (град(w)) и (3) отношение степени к частоте (град(w)/частота(w)) 


Ключевые слова определяются как последовательность слов, следующих друг за другом.
Алгоритм заключается в следующем.

ключевые слова-кандидаты извлекаются путем поиска непрерывной последовательности слов, которая не содержит нерелевантных слов

для каждого слова, входящего в состав любого ключевого слова-кандидата, вычисляется оценка

среди слов-кандидатов алгоритм определяет, сколько раз встречается каждое слово и сколько раз оно встречается совместно с другими словами

каждое слово получает оценку, которая представляет собой отношение степени употребления слова (сколько раз оно встречается в сочетании с другими словами) к частоте употребления слова

итоговый балл для полного ключевого слова-кандидата рассчитывается путем суммирования баллов по каждому из слов, которые определяют ключевое слово-кандидат

Результирующие ключевые слова возвращаются в виде файла data.frame вместе с их оценкой РЕЙКА.



Сравнение ведется по периоду ...?

Описать динамику ...

Описание




% Раздел "Заключение"
\conclusion







%Библиографический список, составленный вручную, без использования BibTeX
%
\begin{thebibliography}{99}
 \bibitem {1} Линник Л. А., Петросян М. М., Облако слов как метод компрессии информации научного текста // Наука. Информатизация. Технологии. Образование : материалы XIII международной научно-практической конференции, г. Екатеринбург, 24-28 февраля 2020 г. ---Екатеринбург : Издательство РГППУ, 2020. --- С. 99-108. 
\bibitem{2} Сапух Татьяна Викторовна Современные средства формирования лексических навыков учащихся на уроках английского языка (на примере облака слов) // АНИ: педагогика и психология. 2018. №3 (24).[Электронный ресурс]. URL: https://cyberleninka.ru/article/n/sovremennye-sredstva-formirovaniya-leksicheskih-navykov-uchaschihsya-na-urokah-angliyskogo-yazyka-na-primere-oblaka-slov (дата обращения: 10.05.2024).
\bibitem{3} Яркин П. А. Изучение экологического дискурса в контексте исследования экологического сознания // Экопсихологические исследования – 6: экология детства и психология устойчивого развития. 2020. №6.[Электронный ресурс]. URL: https://cyberleninka.ru/article/n/izuchenie-ekologicheskogo-diskursa-v-kontekste-issledovaniya-ekologicheskogo-soznaniya (дата обращения: 20.04.2024).
\bibitem{4} Journal of Teaching and Learning with Technology, Vol. 5, No. 1, July 2016, pp.16-32.
\bibitem{5} Umair, A., Masciari, E. Sentimental and spatial analysis of COVID-19 vaccines tweets. J Intell Inf Syst 60, 1–21 (2023). [Электронный ресурс]. URL: https://doi.org/10.1007/s10844-022-00699-4 (дата обращения: 10.05.2024).
\bibitem{6} Liberatore F., Camacho-Collados J. Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction. 2021. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 8089–8103 pp. Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem{7}Gumińska, Urszula \& Poniszewska-Maranda, Aneta \& Ochelska-Mierzejewska, Joanna. (2022). Systematic Comparison of Vectorization Methods in Classification Context. Applied Sciences. 12. 5119. 10.3390/app12105119.  

\bibitem{8}Официальный сайт CRAN для скачивания R для macOS. [Электронный ресурс]. URL:   https://cran.r-project.org/bin/macosx/ (дата обращения: 08.05.2024).







\bibitem{9}Документация библиотеки udpipe. [Электронный ресурс]. URL: https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html (дата обращения: 10.05.2024).



 \bibitem{10} Список русских имён и фамилий. [Электронный ресурс]. URL: https://github.com/Raven-SL/ru-pnames-list/tree/master/lists (дата обращения: 15.04.2024).


\bibitem{11} Официальный сайт shiny. [Электронный ресурс]. URL: https://shiny.posit.co/ (дата обращения: 09.05.2024).

\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
%\bibliographystyle{gost780uv}
%\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением

\begin{center}
    \section*{\makebox[\linewidth]{ПРИЛОЖЕНИЕ А}}
\end{center}
%\section*{ ПРИЛОЖЕНИЕ А}
\addcontentsline{toc}{section}{ПРИЛОЖЕНИЕ А}
\begin{verbatim}
install_or_load_pack <- function(pack){
  create.pkg <- pack[!(pack %in% installed.packages()[, 
  "Package"])]
  if (length(create.pkg))
    install.packages(create.pkg, dependencies = TRUE)
  sapply(pack, require, character.only = TRUE)
}
packages <- c("ggplot2",  "data.table", "wordcloud", "tm", 
"wordcloud2", "tidytext", "devtools", 
"dplyr", 'tidyverse', 'readxl', 
'udpipe', 'writexl', 'xlsx', 'rlang')
install_or_load_pack(packages)

library(shiny)
library(readxl)
library(dplyr)
library(plyr)
library(tidytext)
library(ggplot2)
library(wordcloud2)
library(lsa)


wordcloud2a <- function (data, size = 1, minSize = 0, 
gridSize = 0,
fontFamily = "Segoe UI",               
     fontWeight = "bold", color = "random-dark", 
     backgroundColor = "white", 
     minRotation = -pi/4, maxRotation = pi/4, shuffle = TRUE, 
     rotateRatio = 0.4, shape = "circle", ellipticity = 0.65, 
     widgetsize = NULL, figPath = NULL, hoverFunction = NULL) 
{
  if ("table" %in% class(data)) {
    dataOut = data.frame(name = names(data), freq = 
    as.vector(data))
  }
  else {
    data = as.data.frame(data)
    dataOut = data[, 1:2]
    names(dataOut) = c("name", "freq")
  }
  if (!is.null(figPath)) {
    if (!file.exists(figPath)) {
      stop("cannot find fig in the figPath")
    }
    spPath = strsplit(figPath, "\\.")[[1]]
    len = length(spPath)
    figClass = spPath[len]
    if (!figClass %in% c("jpeg", "jpg", "png", "bmp", "gif")) {
      stop("file should be a jpeg, jpg, png, bmp or gif file!")
    }
    base64 = base64enc::base64encode(figPath)
    base64 = paste0("data:image/", figClass, ";base64,", 
                    base64)
  }
  else {
    base64 = NULL
  }
  weightFactor = size * 180/max(dataOut$freq)
  settings <- list(word = dataOut$name, freq = dataOut$freq, 
                   fontFamily = fontFamily, 
                   fontWeight = fontWeight, 
                   color = color, 
                   minSize = minSize, weightFactor = weightFactor, 
                   backgroundColor = backgroundColor, 
                   gridSize = gridSize, minRotation = minRotation, 
                   maxRotation = maxRotation, 
                   shuffle = shuffle, rotateRatio = rotateRatio, 
                   shape = shape, 
                   ellipticity = ellipticity, figBase64 = base64, 
                   hover = htmlwidgets::JS(hoverFunction))
  chart = htmlwidgets::createWidget("wordcloud2", settings, 
        width = widgetsize[1], 
        height = widgetsize[2], 
        sizingPolicy = htmlwidgets::sizingPolicy(viewer.padding 
        = 0, 
     browser.padding = 0, browser.fill = TRUE))
  chart
}




load_stopwords <- function() {
  female_names_rus <- read.csv("female_names_rus.txt",
  header=FALSE)
  male_names_rus <- read.csv("male_names_rus.txt", header=FALSE)
  male_surnames_rus <- read.csv("male_surnames_rus.txt",
  header=FALSE)
  extra_stop_words <- c('и','димитровграда', 'димитровград',
  'ульяновскаяобласть', 'ульяновск', 'ульяновский', 'саранск', 
  'саранска', 'мордовие', 'рм', 'рма', 'мордовия', 'мордовский', 
  'заец', 'idюрий', 'главамарийэл', 'марий', 'эл', 'марийэл', 
  'эть', 'васил', 'чурин', 'кировский', 'кировскаяобласть',
  'вятский', 'мельниченко', 'месяц', 'оренбургнуть', 
  'объясняемрф', 'провести',  'инвестор', 'вести', 'реализация', 
  'башкортостанный', 'радий', 'подписать', 'проект', 'пермский', 
  'пермскийкрай', 'край', 'прикамья', 'краевой', 'задача', 
  'важно', 'оренбуржец', 'оренбург',  'новость', 'подчеркнуть', 
  'оренбуржье', 'оренбургский', 
  'оренбургскаяобласть', 'поддержка', 'часть', 'км', 
  'валерийрадаеть', 'олегнуть', 'должен', 'около', 'рассказать', 
  'глава', 'губернатор', 'развитие', 'январь', 'февраль', 'март', 
  'апрель', 'май', 'июнь', 'июль', 'август', 'сентябрь', 
  'октябрь', ноябрь', 'декабрь', 'город', 'ecom', 'казань', 
  'подробность', 'подробный', 'радия', 'процент', 
  'уф', 'часть', 'вопрос',  'делать',  'сделать',
  'благодаря', 'участие', 'пройти', 'идти', 'создать', 
  'создавать', 'дать',  'рамка', 'место', 'первый', 'получить', 
  'удмуртия', 'радай', 'юлие', 'пензенский', 'пенза', 
  'пензенскаяобласть', 'новый', 'лучший', 'самый', 'работа', 
  'рабочий', 'работать', 'региональный', 'нижегородскаяобладать', 
  'clubнижегородский', 'нижегородскаяобласть', 'нижегородский', 
  'нижний', 'новгород', 'чувашия', 'чувашие', 'обть', 
  "бaшҡортостать", "бaшҡортостан", 'командахабиров', 'рб', 
  'миллиард', 'башкирия', 'башкортостан', 'башкортостана', 
  'мый', 'аный', 'мухаметшина', 'мухаметшин', 'реть', 
  'рф', 'день', 'отметить', 'число', 'миллион', 'ход', 
  'президент','страна',  'тысяча', 'рубль', 'доллар', 
  'район', 'итог', 'татарстан', 
  'татарстать', 'российский', 'ма', 'область', 'республика', 
  'саратовский', 'татарстан', 'татарстана', 
  'самарский','экономический', 'экономика', 'регион', 'год', 
  "миннихан", "рт", "россия", "рустам", "руст", 'россия', 
  'конкурентоспособность', 'инновация', 'инвестиция', 
  'инвестиционный', 'рустамминнихан', 'дмитрий', 'азаров', 
  'саратовскаяобласть', 'саратовская', 'самарскаяобласть', 
  'азар', 'стать', 'rn«', 'твой', 'сих', 'ком', 'свой',
    'слишком', 'нами', 'всему', 'будь', 'саму', 
    'чаще', 'ваше', 'наш', 'затем', 'еще', 
    'наши', 'ту', 'каждый',
    'мочь', 'весь', 'этим', 'наша', 'своих', 
    'оба', 'который', 'зато', 'те', 'вся', 'ваш', 
    'такая', 'теми', 'ею', 'нередко',
    'также', 'чему', 'собой', 'нем', 'вами', 
    'ими', 'откуда', 'такие', 'тому', 'та', 
    'очень', 'нему',  'д', 'алло', 'оно', 'кому', 
    'тобой', 'таки', 'мой', 'нею', 'ваши', 
    'ваша', 'кем', 'мои', 'однако', 'сразу', 'свое', 
    'ними', 'всё', 'неё', 'тех', 'хотя', 
    'всем', 'тобою', 'тебе', 'одной', 'другие',
    'буду', 'моё', 'своей', 'такое', 'всею', 'будут', 
    'своего',  'кого', 'свои', 'мог', 'нам', 'особенно', 
    'её','наше', 'кроме', 'вообще', 'вон', 'мною', 'никто', 
    'это', 'изза', 'именно', 'поэтому', 'будьт', 'являться', 
    'чувашский', 'тыса', 'смочь', 'ваший', 'гльба', 
    'ать', 'уть', 'ивать', 'ольги', 'пенз', 'ер', 'иметь', 
    'олегнуть', 'сг', 'например', 'сообщить', 
    'сообщать', 'среди', 'нть', 'пер', 'зспермь', 'края', 
    'ради',  'назвать', 'важный')
  stopwords_combined <- paste(c(stopwords("russian"), 
  extra_stop_words,
                                tolower(male_names_rus$V1),
                                tolower(male_surnames_rus$V1),
                                tolower(female_names_rus$V1)),
                                collapse = "|")
}

stopwords_combined <- load_stopwords()

ui <- fluidPage(
  titlePanel("Анализ регионов по разным периодам"),
  tabsetPanel(id = "tabs",
              tabPanel("Период 1",
                       sidebarLayout(
                         sidebarPanel(
                           fileInput("file1",
                           "Выберите Excel файл 1", accept = 
                           ".xlsx"),
                           actionButton("analyze1",
                           "Анализировать файл 1"),
                           
                         ),
                         mainPanel(
                           plotOutput("barPlot1"),
                           wordcloud2Output("wordcloud1"),
                           tableOutput("wordTable1")
                         )
                       )
              ),
              tabPanel("Период 2",
                       sidebarLayout(
                         sidebarPanel(
                           fileInput("file2",
                           "Выберите Excel файл 2", accept = 
                           ".xlsx"),
                           actionButton("analyze2", 
                           "Анализировать файл 2"),
                          
                         ),
                         mainPanel(
                           plotOutput("barPlot2"),
                           wordcloud2Output("wordcloud2"),
                           tableOutput("wordTable2")
                         )
                       )
              ),
              tabPanel("Период 3",
                       sidebarLayout(
                         sidebarPanel(
                           fileInput("file3", 
                           "Выберите Excel файл 3", accept = 
                           ".xlsx"),
                           actionButton("analyze3",
                           "Анализировать файл 3"),
                           
                         ),
                         mainPanel(
                           plotOutput("barPlot3"),
                           wordcloud2Output("wordcloud3"),
                           tableOutput("wordTable3")
                         )
                       )
              ),
              tabPanel("Сравнить файлы",
                       
                       actionButton("compare_files_btn", 
                       "Сравнить введенные файлы"),
                       tableOutput("compare_files_table")
              )
  )
)

server <- function(input, output, session) {
  files_preprocessed_data <- reactiveValues()
  clean_corpus <- function(corpus_to_use){
    corpus_to_use %>%
      tm_map(removePunctuation) %>%
      tm_map(stripWhitespace) %>%
      tm_map(content_transformer(function(x) 
      iconv(x, to='UTF-8'))) %>%
      tm_map(removeNumbers) %>%
      tm_map(content_transformer(tolower)) 
  }
  get_preprocessed_texts_word_list <- function(file) {
    req(file)
    input_data <- read_excel(file$datapath)
    load_stopwords()
    corp_city_df <- clean_corpus(VCorpus(VectorSource(
    input_data)))
    corp_city_df[["1"]][["content"]] <- gsub("[
    \U{1F600}-\U{1F64F}
    \U{1F300}-\U{1F5FF}\U{1F680}-\U{1F6FF}\U{1F1E0}-\U{1F1FF}
    \U{2500}-\U{2BEF}\U{2702}-\U{27B0}\U{24C2}-\U{1F251}
    \U{1f926}-\U{1f937}\U{10000}-\U{10ffff}\u{2640}-\u{2642}
    \u{2600}-\u{2B55}\u{200d}\u{23cf}\u{23e9}\u{231a}\u{fe0f}
    \u{3030}]","", corp_city_df[["1"]][["content"]], 
    perl = TRUE)
    if (!file.exists('russian-gsd-ud-2.5-191206.udpipe'))
    {
      gsd_model_raw <- udpipe_download_model(language = 
      "russian-gsd")
    }
    gsd_model <- udpipe_load_model(file =
      'russian-gsd-ud-2.5-191206.udpipe')
    x <- udpipe_annotate(gsd_model, x = corp_city_df[["1"]]
      [["content"]],  parser = "none")
    x <- as.data.frame(x)
    x$lemma <- noquote(x$lemma)
    x$lemma <- str_replace_all(x$lemma, "[[:punct:]]", "")
    tmp <- x$lemma
    tmp <- str_replace_all(x$lemma, paste("\\b(", 
    stopwords_combined,
    ")\\b"), "")
    tmp <- str_replace_all(tmp, '№', '')
    tmp <- str_replace_all(tmp, '−', '')
    tmp <- str_replace_all(tmp, '—', '')
    tmp <- str_replace_all(tmp, 'правительстворазвитие', 
    'правительство')
    tmp <- str_replace_all(tmp, 'правительстворб', 
      'правительство')
    tmp <- str_replace_all(tmp, 'цифровый', 'цифровой')
    tmp <- str_replace_all(tmp, 'научныймощность', 
    'научный мощность')
    tmp <- str_replace_all(tmp, 'club', '')
    tmp <- str_replace_all(tmp, 'правительствомарийэть', 
      'правительство')
    tmp <- str_replace_all(tmp, 'цура', 'цур')
    tmp <- tmp[sapply(tmp, nchar) > 0]
    return(tmp)
  }
  analyze_and_render <- function(file_input, 
  plot_output, table_output, wordcloud_output) {
    preprocessed_texts_word_list <- 
      get_preprocessed_texts_word_list(file_input)
    d <- as.data.frame(sort(table(
    preprocessed_texts_word_list), decreasing = TRUE))
    colnames(d) <- c("word", "freq")
    word_freq <- d
    d$tf <- d$freq / nrow(d)
    output[[plot_output]] <- renderPlot({
      ggplot(word_freq[1:10, ], 
      aes(x = reorder(word, freq), y = freq)) +
        geom_bar(stat = "identity") +
        coord_flip() +
        labs(title = "Наиболее часто встречающиеся слова", 
          x = "Слова", y = "Частота встречаемости") +
        theme_gray(base_size = 18)
    })
    output[[table_output]] <- renderTable({
      colnames(word_freq) <- c("Слово", 
        "Частота встречаемости слова в корпусе текстов")
      head(word_freq, 10)
    })
    output[[wordcloud_output]] <- renderWordcloud2({
      wordcloud2a(word_freq, size = 0.45)
    })
    return(d)
  }
  observeEvent(input$analyze1, {
    files_preprocessed_data[["df_1"]] <- 
      analyze_and_render(input[["file1"]],
      "barPlot1", "wordTable1", "wordcloud1")
  })
  observeEvent(input$analyze2, {
    files_preprocessed_data[["df_2"]] <- 
      analyze_and_render(input[["file2"]], 
      "barPlot2", "wordTable2", "wordcloud2")
  })
  observeEvent(input$analyze3, {
    files_preprocessed_data[["df_3"]] <- 
    analyze_and_render(input[["file3"]], 
    "barPlot3", "wordTable3", "wordcloud3")
  })
  observeEvent(input[["compare_files_btn"]], {
    d_all <- Filter(Negate(is.null), 
    list(files_preprocessed_data[["df_1"]], 
    files_preprocessed_data[["df_2"]], 
    files_preprocessed_data[["df_3"]])) 
    cos.mat <- NULL
    if (length(d_all) == 1) {
    }
    if (length(d_all) == 2) {
      d_all <- full_join(d_all[[1]], d_all[[2]], by='word')
      d_all <- d_all %>% replace(is.na (.), 0)
      tf_idf <- select(d_all, 'word', 'freq.x', 'tf.x',
      'freq.y','tf.y')
      names(tf_idf) <- c('word', 'freq1', 'tf1', 'freq2', 
      'tf2')
      tdm_df <- select(d_all, 'word', 'freq.x', 'freq.y')
      names(tdm_df) <- c('word', 'freq1', 'freq2')
      tdm_df <- tdm_df %>% mutate(num_of_occurrences = 
      rowSums(select(tdm_df, 'freq1', 'freq2') != 0))
      tdm_df <- tdm_df %>% mutate(idf = log(4 / 
      (1 + num_of_occurrences) + 1))
      tf_idf <- tf_idf %>% mutate(num_of_occurrences = 
      tdm_df$num_of_occurrences)
      tf_idf <- tf_idf %>% mutate(idf = tdm_df$idf)
      tf_idf <- tf_idf %>% mutate(tf_idf1 = tf1 * idf)
      tf_idf <- tf_idf %>% mutate(tf_idf2 = tf2 * idf)
      tf_idf_only <- select(tf_idf, 'tf_idf1', 'tf_idf2')
      names(tf_idf_only) <- c("Период 1", "Период 2")
      cos.mat <- cosine(as.matrix(tf_idf_only))  
    }
    if (length(d_all) == 3) {
      d_all <- full_join(full_join(d_all[[1]], 
      d_all[[2]], by='word'), d3, by='word')
      d_all <- d_all %>% replace(is.na (.), 0)
      tf_idf <- select(d_all, 'word', 'freq.x', 'tf.x', 
      'freq.y','tf.y', 'freq', 'tf')
      names(tf_idf) <- c('word', 'freq1', 'tf1', 'freq2',
      'tf2', 'freq3', 'tf3')
      tdm_df <- select(d_all, 'word', 'freq.x', 'freq.y',
      'freq')
      names(tdm_df) <- c('word', 'freq1', 'freq2', 'freq3')
      tdm_df <- tdm_df %>% mutate(num_of_occurrences = 
      rowSums(select(tdm_df, 'freq1', 'freq2', 'freq3') 
      != 0))
      tdm_df <- tdm_df %>% mutate(idf = log(4 / 
      (1 + num_of_occurrences) + 1))
      tf_idf <- tf_idf %>% mutate(num_of_occurrences = 
      tdm_df$num_of_occurrences)
      tf_idf <- tf_idf %>% mutate(idf = tdm_df$idf)
      tf_idf <- tf_idf %>% mutate(tf_idf1 = tf1 * idf)
      tf_idf <- tf_idf %>% mutate(tf_idf2 = tf2 * idf)
      tf_idf <- tf_idf %>% mutate(tf_idf3 = tf3 * idf)
      tf_idf_only <- select(tf_idf, 'tf_idf1', 
      'tf_idf2', 'tf_idf3')
      names(tf_idf_only) <- c("Период 1", "Период 2",
      "Период 3")
      cos.mat <- cosine(as.matrix(tf_idf_only))
    }
    output$compare_files_table <- renderTable({
      cos.mat
    })
  })
}

shinyApp(ui = ui, server = server)

\end{verbatim}
\end{document}
